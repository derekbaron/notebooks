{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec on Bible Data\n",
    "\n",
    "Playing around with word embeddings using the Christian Bible as a data source.\n",
    "\n",
    "1. Read bible data into table. 1 verse per row.\n",
    "2. Normalize verses into tokens\n",
    "3. Train w2v model on the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>v</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8593</th>\n",
       "      <td>10021013</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>and he brought up from there the bones of Saul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15200</th>\n",
       "      <td>19080002</td>\n",
       "      <td>19</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>Before Ephraim and Benjamin and Manasseh, stir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15807</th>\n",
       "      <td>19112004</td>\n",
       "      <td>19</td>\n",
       "      <td>112</td>\n",
       "      <td>4</td>\n",
       "      <td>Light dawns in the darkness for the upright, G...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id   b    c   v  \\\n",
       "8593   10021013  10   21  13   \n",
       "15200  19080002  19   80   2   \n",
       "15807  19112004  19  112   4   \n",
       "\n",
       "                                                       t  \n",
       "8593   and he brought up from there the bones of Saul...  \n",
       "15200  Before Ephraim and Benjamin and Manasseh, stir...  \n",
       "15807  Light dawns in the darkness for the upright, G...  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, spacy, gensim, re\n",
    "bible = pd.read_csv(\"https://github.com/scrollmapper/bible_databases/raw/master/csv/t_web.csv\")\n",
    "bible.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The verse texts are in the \"t\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['beginning', 'god', 'god', 'hebrew', 'letter', 'aleph', 'tav', 'letter', 'hebrew', 'alphabet', 'grammatical', 'marker', 'create', 'heavens', 'earth'], ['earth', 'formless', 'darkness', 'surface', 'deep', 'god', 'spirit', 'hover', 'surface', 'water']]\n"
     ]
    }
   ],
   "source": [
    "# Using spacy NLP to normalize/lemmatize/tokenize the verses.\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "def cleaning(doc):\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    if len(txt) > 2:\n",
    "        return txt\n",
    "# Remove punctuation, make everything lowercase\n",
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in bible.t)\n",
    "corp = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1) if doc != None]\n",
    "corp = [n for n in corp if n != None]\n",
    "print(corp[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(min_count=5,\n",
    "                     window=10,\n",
    "                     size=200,\n",
    "                     sample=0, \n",
    "                     alpha=0.025, min_alpha=0.001,\n",
    "                     workers=1)\n",
    "\n",
    "w2v_model.build_vocab(corp, progress_per=10000, update=False)\n",
    "w2v_model.train(corp, total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs)\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Let's examine the results. We'll look at the similarity between words.\n",
    "\n",
    " First, find the terms most similar to \"Jesus\" and \"Christ.\" The pairings are very interesting. \"Ask\" with \"answer.\" \"Listen\" with \"testify.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ask', 0.8884044885635376),\n",
       " ('answer', 0.87091463804245),\n",
       " ('listen', 0.8665204048156738),\n",
       " ('testify', 0.8645660877227783),\n",
       " ('believe', 0.8633058071136475),\n",
       " ('gospel', 0.8624488115310669),\n",
       " ('inquire', 0.8571044206619263),\n",
       " ('fellowship', 0.842028021812439),\n",
       " ('think', 0.8410632610321045),\n",
       " ('understand', 0.8329905271530151)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"jesus\", \"christ\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar with Joy\n",
    "What's semantically similar with joy? **Salvation, justice, hope**. Good things. Joyful things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('salvation', 0.9415661096572876),\n",
       " ('justice', 0.934103786945343),\n",
       " ('glad', 0.9299027919769287),\n",
       " ('hope', 0.9257656335830688),\n",
       " ('world', 0.9226891994476318),\n",
       " ('selah', 0.9220409393310547),\n",
       " ('perfect', 0.9212071895599365),\n",
       " ('mercy', 0.9183230996131897),\n",
       " ('righteousness', 0.9138761758804321),\n",
       " ('truth', 0.899572491645813)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"rejoice\", \"joy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doesn't match\n",
    "Which of these words is not like the others? ***Anger*** is certainly not one of the fruits of the spirit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anger'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match(['love', 'anger', 'joy', 'patience', 'peace'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy\n",
    "\n",
    "**Covenant** is to **promise** as **law** is to... **commandment** or *statue* or *ordinance*.\n",
    " \n",
    " I really like these results. They seem to capture the essence of the concept of \"law.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('commandment', 0.8345969915390015),\n",
       " ('statute', 0.8313448429107666),\n",
       " ('ordinance', 0.83086758852005),\n",
       " ('accord', 0.7626932263374329),\n",
       " ('keep', 0.7570266723632812)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"covenant\", \"law\"], negative=[\"promise\"], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apostles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('john', 0.9550307989120483),\n",
       " ('james', 0.9499552249908447),\n",
       " ('zebedee', 0.9353220462799072),\n",
       " ('judas', 0.9187829494476318),\n",
       " ('iscariot', 0.9185448288917542)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"peter\", \"simon\", \"andrew\"])[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
